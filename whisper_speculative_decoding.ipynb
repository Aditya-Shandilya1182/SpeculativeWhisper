{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya-Shandilya1182/SpeculativeWhisper/blob/main/whisper_speculative_decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDKgnLyQU7Vh"
      },
      "source": [
        "# Downloads And Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iRGr6Zv-8qO"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U openai-whisper\n",
        "!pip install -q -U jiwer\n",
        "!wget -q https://www.openslr.org/resources/12/dev-clean.tar.gz\n",
        "!tar -xf dev-clean.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBAfSRNI_I-w"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from jiwer import wer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0jfEa0uVDGw"
      },
      "source": [
        "# Speculative Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX3fnNm_Pqal"
      },
      "outputs": [],
      "source": [
        "class SpeculativeWhisper:\n",
        "    def __init__(self, config):\n",
        "        self.device = config.device\n",
        "        self.draft = whisper.load_model(config.draft_model).to(self.device)\n",
        "        self.final = whisper.load_model(config.final_model).to(self.device)\n",
        "        self.tokenizer = whisper.tokenizer.get_tokenizer(multilingual=True)\n",
        "        self.k = config.k\n",
        "        self.max_tokens = config.max_tokens\n",
        "        self.mel_dim_tiny = config.mel_dim_tiny\n",
        "        self.mel_dim_large = config.mel_dim_large\n",
        "        self.beam_search = config.beam_search\n",
        "        self.beam_size = getattr(config, \"beam_size\", 5)\n",
        "        self.top_p = getattr(config, \"top_p\", None)\n",
        "\n",
        "    def topp_sample(self, logits):\n",
        "        if self.top_p is not None:\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "            cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
        "            mask = cumsum > self.top_p\n",
        "            mask[..., 1:] = mask[..., :-1].clone()\n",
        "            mask[..., 0] = False\n",
        "            sorted_probs[mask] = 0.0\n",
        "            sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
        "            idx = torch.multinomial(sorted_probs, 1)\n",
        "            return sorted_idx.gather(-1, idx).squeeze(-1)\n",
        "        return torch.argmax(logits, dim=-1)\n",
        "\n",
        "    def _beam_search(self, tmp, draft_encoder, active_indices):\n",
        "        beams = [(tmp, torch.zeros(tmp.size(0), device=self.device))]\n",
        "        for _ in range(self.k):\n",
        "            new_beams = []\n",
        "            for seq, score in beams:\n",
        "                logits = self.draft.decoder(seq, draft_encoder[active_indices])[:, -1]\n",
        "                logp = F.log_softmax(logits, dim=-1)\n",
        "                topk_logp, topk_idx = torch.topk(logp, self.beam_size, dim=-1)\n",
        "                for b in range(self.beam_size):\n",
        "                    nt = topk_idx[:, b]\n",
        "                    ns = torch.cat([seq, nt[:, None]], dim=1)\n",
        "                    new_beams.append((ns, score + topk_logp[:, b]))\n",
        "            beams = sorted(new_beams, key=lambda x: x[1].sum().item(), reverse=True)[:self.beam_size]\n",
        "        best = beams[0][0]\n",
        "        draft = []\n",
        "        for i in range(self.k):\n",
        "            draft.append(best[:, -(self.k - i)])\n",
        "        return torch.stack(draft, dim=1), best\n",
        "\n",
        "    def decode(self, draft_encoder, final_encoder, max_tokens=None):\n",
        "        max_tokens = max_tokens or self.max_tokens\n",
        "        batch = final_encoder.size(0)\n",
        "        tokens = [torch.full((1,), self.tokenizer.sot, device=self.device, dtype=torch.long) for _ in range(batch)]\n",
        "        done = torch.zeros(batch, dtype=torch.bool, device=self.device)\n",
        "\n",
        "        for _ in range(max_tokens):\n",
        "            active_indices = (~done).nonzero(as_tuple=True)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            tmp = torch.nn.utils.rnn.pad_sequence(\n",
        "                [tokens[i] for i in active_indices],\n",
        "                batch_first=True,\n",
        "                padding_value=self.tokenizer.sot,\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.beam_search:\n",
        "                    draft, tmp = self._beam_search(tmp, draft_encoder, active_indices)\n",
        "                else:\n",
        "                    draft_list = []\n",
        "                    for _ in range(self.k):\n",
        "                        logits = self.draft.decoder(tmp, draft_encoder[active_indices])[:, -1]\n",
        "                        next_tok = self.topp_sample(logits)\n",
        "                        draft_list.append(next_tok)\n",
        "                        tmp = torch.cat([tmp, next_tok[:, None]], dim=1)\n",
        "                    draft = torch.stack(draft_list, dim=1)\n",
        "\n",
        "            verify = torch.cat([tmp[:, :-draft.size(1)], draft[:, :-1]], dim=1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = self.final.decoder(verify, final_encoder[active_indices])\n",
        "                logp = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            for idx, seq_idx in enumerate(active_indices):\n",
        "                accepted = 0\n",
        "                base = tokens[seq_idx].size(0) - 1\n",
        "                for i in range(draft.size(1)):\n",
        "                    pred = torch.argmax(logp[idx, base + i], dim=-1)\n",
        "                    if pred == draft[idx, i]:\n",
        "                        accepted += 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                if accepted > 0:\n",
        "                    tokens[seq_idx] = torch.cat([tokens[seq_idx], draft[idx, :accepted]], dim=0)\n",
        "\n",
        "                if accepted < draft.size(1):\n",
        "                    pos = tokens[seq_idx].size(0) - 1\n",
        "                    fb = self.topp_sample(logp[idx, pos])\n",
        "                    tokens[seq_idx] = torch.cat([tokens[seq_idx], fb.unsqueeze(0)], dim=0)\n",
        "\n",
        "                done[seq_idx] = tokens[seq_idx][-1] == self.tokenizer.eot\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def transcribe(self, audio_files, max_tokens=None):\n",
        "        max_tokens = max_tokens or self.max_tokens\n",
        "        audios = []\n",
        "\n",
        "        for p in audio_files:\n",
        "            a = whisper.load_audio(p)\n",
        "            a = whisper.pad_or_trim(a)\n",
        "            audios.append(torch.from_numpy(a))\n",
        "\n",
        "        audios = torch.stack(audios).to(self.device)\n",
        "        mel_tiny = torch.stack([whisper.log_mel_spectrogram(a, self.mel_dim_tiny) for a in audios]).to(self.device)\n",
        "        mel_large = torch.stack([whisper.log_mel_spectrogram(a, self.mel_dim_large) for a in audios]).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            draft_encoder = self.draft.encoder(mel_tiny)\n",
        "            final_encoder = self.final.encoder(mel_large)\n",
        "\n",
        "        batch_tokens = self.decode(draft_encoder, final_encoder, max_tokens)\n",
        "\n",
        "        return [self.tokenizer.decode(t.tolist()) for t in batch_tokens]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0mmNs6RVN8f"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbMcDzNCLO3_"
      },
      "outputs": [],
      "source": [
        "def load_references(audio_files):\n",
        "    refs = []\n",
        "    for path in audio_files:\n",
        "        base = os.path.basename(path).replace(\".flac\", \"\")\n",
        "        chapter_dir = os.path.dirname(path)\n",
        "        speaker_id = os.path.basename(os.path.dirname(chapter_dir))\n",
        "        chapter_id = os.path.basename(chapter_dir)\n",
        "        txt_path = os.path.join(chapter_dir, f\"{speaker_id}-{chapter_id}.trans.txt\")\n",
        "        with open(txt_path) as f:\n",
        "            for line in f:\n",
        "                if line.startswith(base):\n",
        "                    refs.append(line.strip().split(\" \", 1)[1].lower())\n",
        "                    break\n",
        "    return refs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta7GCALcN6Pm"
      },
      "outputs": [],
      "source": [
        "def clean_whisper_output(text):\n",
        "    text = re.sub(r\"<\\|.*?\\|>\", \"\", text)\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZlhvtj-VSuK"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psR9xHYWDbpX"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    draft_model: str = \"tiny\"\n",
        "    final_model: str = \"large-v3\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    k: int = 5\n",
        "    beam_search: bool = False\n",
        "    beam_size: int = 4\n",
        "    top_p: float | None = 0.9\n",
        "    max_tokens: int = 200\n",
        "    mel_dim_tiny: int = 80\n",
        "    mel_dim_large: int = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U1tjfmbVWbN"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs0Ula1-QpPe"
      },
      "outputs": [],
      "source": [
        "audio_files = [\n",
        "    \"LibriSpeech/dev-clean/84/121123/84-121123-0000.flac\",\n",
        "    \"LibriSpeech/dev-clean/84/121123/84-121123-0001.flac\",\n",
        "    \"LibriSpeech/dev-clean/84/121123/84-121123-0002.flac\",\n",
        "    \"LibriSpeech/dev-clean/84/121123/84-121123-0003.flac\",\n",
        "    \"LibriSpeech/dev-clean/84/121123/84-121123-0004.flac\",\n",
        "]\n",
        "\n",
        "references = load_references(audio_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzvUr0PbDcpf"
      },
      "outputs": [],
      "source": [
        "sw = SpeculativeWhisper(Config())\n",
        "\n",
        "spec_times = []\n",
        "spec_outputs = []\n",
        "\n",
        "for path in audio_files:\n",
        "    start = time.time()\n",
        "    out = sw.transcribe([path], max_tokens=100)[0]\n",
        "    torch.cuda.synchronize()\n",
        "    t = time.time() - start\n",
        "\n",
        "    spec_times.append(t)\n",
        "    spec_outputs.append(out)\n",
        "\n",
        "spec_preds = [clean_whisper_output(o) for o in spec_outputs]\n",
        "spec_wer = wer(references, spec_preds)\n",
        "\n",
        "print(\"\\n--- Speculative Transcription Outputs ---\")\n",
        "\n",
        "for i, out in enumerate(spec_preds):\n",
        "    print(f\"\\n[{i}] {out}\")\n",
        "\n",
        "print(\"\\nSpeculative Whisper\")\n",
        "print(f\"Avg latency/sample: {sum(spec_times)/len(spec_times):.4f}s\")\n",
        "print(f\"Min latency: {min(spec_times):.4f}s\")\n",
        "print(f\"Max latency: {max(spec_times):.4f}s\")\n",
        "print(f\"Total time: {sum(spec_times):.2f}s\")\n",
        "print(f\"WER: {spec_wer:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKpMh0b5Rdg-"
      },
      "outputs": [],
      "source": [
        "del sw\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N99K1XC0LbjN"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "vanilla_model = whisper.load_model(\"large-v3\").to(device)\n",
        "\n",
        "vanilla_times = []\n",
        "vanilla_outputs = []\n",
        "\n",
        "for path in audio_files:\n",
        "    start = time.time()\n",
        "    r = vanilla_model.transcribe(path, language=\"en\")\n",
        "    torch.cuda.synchronize()\n",
        "    t = time.time() - start\n",
        "\n",
        "    vanilla_times.append(t)\n",
        "    vanilla_outputs.append(r[\"text\"])\n",
        "\n",
        "vanilla_wer = wer(references, vanilla_outputs)\n",
        "\n",
        "print(\"\\n--- Vanilla Transcription Outputs ---\")\n",
        "\n",
        "for i, out in enumerate(vanilla_outputs):\n",
        "    print(f\"\\n[{i}] {out}\")\n",
        "\n",
        "print(\"\\nVanilla Whisper Large-V3\")\n",
        "print(f\"Avg latency/sample: {sum(vanilla_times)/len(vanilla_times):.4f}s\")\n",
        "print(f\"Min latency: {min(vanilla_times):.4f}s\")\n",
        "print(f\"Max latency: {max(vanilla_times):.4f}s\")\n",
        "print(f\"Total time: {sum(vanilla_times):.2f}s\")\n",
        "print(f\"WER: {vanilla_wer:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMX+D+4/sDRQfamUY+SWqU5",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
